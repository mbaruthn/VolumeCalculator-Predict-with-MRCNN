{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaynakları Yükle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Python 3.10.10 enviromentını kurduktan sonra, torch2.0.1+cu118 PyTorch paketini de kurmamız gerekmektedir.\n",
    "\n",
    "Pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu118 \n",
    "* Gerekli kaynakları, pip install -r requirements.txt kodu ile python kütüphanesine yüklediğinizden emin olunuz.\n",
    "* Bu proje içerisinde örnek model bulunmamaktadır. \n",
    "* En doğru sonuçların alınabilmesi için, VolumeCalculator-Training ile eğitilen model kullanılmalıdır.\n",
    " -------------------------------------------------------------\n",
    "* After installing Python 3.10.10 environment, it is necessary to install the torch2.0.1+cu118 PyTorch package as well.\n",
    "    \n",
    "pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu118\n",
    "* Make sure you have installed the necessary dependencies into the Python library using the following command:\n",
    "    \n",
    "pip install -r requirements.txt\n",
    "* There is no sample model in project.\n",
    "* To obtain the most accurate results, the model trained with VolumeCalculator-Training should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import Metadata\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import DefaultPredictor\n",
    "import yaml\n",
    "import numpy\n",
    "from aioflask import Flask, jsonify, request\n",
    "import time\n",
    "from PIL import Image\n",
    "import asyncio\n",
    "import base64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTime():\n",
    "    # Get the current timestamp in seconds\n",
    "    current_timestamp = time.time()\n",
    "\n",
    "    # Extract the current minute, second, and millisecond\n",
    "    current_minute = int((current_timestamp // 60) % 60)\n",
    "    current_second = int(current_timestamp % 60)\n",
    "    current_millisecond = int((current_timestamp % 1) * 1000)\n",
    "\n",
    "    # Format the time as string (mm:ss.sss)\n",
    "    currentTime = f\"{current_minute:02}:{current_second:02}.{current_millisecond:03}\"\n",
    "    return currentTime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volume Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateVolume(x, y, x2, y2, dpi, pixelcount):\n",
    "    \n",
    "    # Define the data points for the volume calculation\n",
    "    volx = numpy.array([148020, 174503, 374399, 477370, 662724])\n",
    "    voly = numpy.array([250, 330, 1000, 1500, 2500])\n",
    "\n",
    "    # Set the degree of the polynomial equation\n",
    "    polynomial_degree = 2\n",
    "\n",
    "    # Calculate coefficients of the polynomial equation\n",
    "    coefficients = numpy.polyfit(volx, voly, polynomial_degree)\n",
    "\n",
    "    # Create the polynomial equation\n",
    "    poly_eq = numpy.poly1d(coefficients)\n",
    "\n",
    "    # Specify the pixel count for prediction\n",
    "    new_x = pixelcount\n",
    "\n",
    "    # Predict the corresponding y value using the polynomial equation\n",
    "    predicted_y = poly_eq(new_x)\n",
    "    \n",
    "    # Round the predicted volume\n",
    "    volume = round(predicted_y)\n",
    "\n",
    "    # Calculate the difference in pixels\n",
    "    x = x2 - x\n",
    "    y = y2 - y\n",
    "\n",
    "    # Convert pixel differences to centimeters using DPI (dots per inch) value\n",
    "    x = x * 2.54 / dpi\n",
    "    y = y * 2.54 / dpi\n",
    "\n",
    "    # Calculate the diameter of the mask in centimeters\n",
    "    diameter = y / 2\n",
    "    diameter = diameter * diameter\n",
    "    \n",
    "    # Adjust dimensions based on specific conditions\n",
    "    if not (x <= 0 and y <= 0):\n",
    "        x = round(x * 10, 1)  # Convert to millimeters and round to one decimal place\n",
    "        y = round(y * 10, 1)  # Convert to millimeters and round to one decimal place\n",
    "        x = float(x) * 1.554   # Apply a scaling factor\n",
    "        x = float(x) - 77      # Adjust by a constant value\n",
    "        y = float(y) * 1.134   # Apply a scaling factor\n",
    "        y = float(y) - 2.7     # Adjust by a constant value\n",
    "    else:\n",
    "        x = 0\n",
    "        y = 0\n",
    "\n",
    "    # Calculate the mask dimensions in millimeters\n",
    "    mask_dimensions_mm = {\"x\": round(x * 1), \"y\": round(y * 1)}\n",
    "\n",
    "    return volume, mask_dimensions_mm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_and_show_image(image, instance, filename):\n",
    "    \n",
    "    # Create a Visualizer object with the input image and instance predictions\n",
    "    image_visualizer = Visualizer(image[:, :, ::-1], metadata=metadata, scale=1)\n",
    "    image_with_bb = image_visualizer.draw_instance_predictions(instance.to(\"cpu\"))\n",
    "\n",
    "    # Extract the output image from the Visualizer object and perform BGR to RGB conversion\n",
    "    output_image = image_with_bb.get_image()[:, :, ::-1]\n",
    "\n",
    "    # Display the output image\n",
    "    cv2.imshow(\"image\", output_image)\n",
    "    cv2.waitKey()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We specify the path to the metadata.yaml file because we will retrieve class names from the Metadata file. Then, we create a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find metadata file and load\n",
    "with open(\"./model/metadata.yaml\", \"r\") as f:\n",
    "    metadata_dict = yaml.load(f, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can continue to create dictionary if yaml loaded successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "namespace(thing_classes=['bottle'],\n",
       "          thing_colors=[[220, 20, 60],\n",
       "                        [119, 11, 32],\n",
       "                        [0, 0, 142],\n",
       "                        [0, 0, 230],\n",
       "                        [106, 0, 228],\n",
       "                        [0, 60, 100],\n",
       "                        [0, 80, 100],\n",
       "                        [0, 0, 70],\n",
       "                        [0, 0, 192],\n",
       "                        [250, 170, 30],\n",
       "                        [100, 170, 30],\n",
       "                        [220, 220, 0],\n",
       "                        [175, 116, 175],\n",
       "                        [250, 0, 30],\n",
       "                        [165, 42, 42],\n",
       "                        [255, 77, 255],\n",
       "                        [0, 226, 252],\n",
       "                        [182, 182, 255],\n",
       "                        [0, 82, 0],\n",
       "                        [120, 166, 157],\n",
       "                        [110, 76, 0],\n",
       "                        [174, 57, 255],\n",
       "                        [199, 100, 0],\n",
       "                        [72, 0, 118],\n",
       "                        [255, 179, 240],\n",
       "                        [0, 125, 92],\n",
       "                        [209, 0, 151],\n",
       "                        [188, 208, 182],\n",
       "                        [0, 220, 176],\n",
       "                        [255, 99, 164],\n",
       "                        [92, 0, 73],\n",
       "                        [133, 129, 255],\n",
       "                        [78, 180, 255],\n",
       "                        [0, 228, 0],\n",
       "                        [174, 255, 243],\n",
       "                        [45, 89, 255],\n",
       "                        [134, 134, 103],\n",
       "                        [145, 148, 174],\n",
       "                        [255, 208, 186],\n",
       "                        [197, 226, 255],\n",
       "                        [171, 134, 1],\n",
       "                        [109, 63, 54],\n",
       "                        [207, 138, 255],\n",
       "                        [151, 0, 95],\n",
       "                        [9, 80, 61],\n",
       "                        [84, 105, 51],\n",
       "                        [74, 65, 105],\n",
       "                        [166, 196, 102],\n",
       "                        [208, 195, 210],\n",
       "                        [255, 109, 65],\n",
       "                        [0, 143, 149],\n",
       "                        [179, 0, 194],\n",
       "                        [209, 99, 106],\n",
       "                        [5, 121, 0],\n",
       "                        [227, 255, 205],\n",
       "                        [147, 186, 208],\n",
       "                        [153, 69, 1],\n",
       "                        [3, 95, 161],\n",
       "                        [163, 255, 0],\n",
       "                        [119, 0, 170],\n",
       "                        [0, 182, 199],\n",
       "                        [0, 165, 120],\n",
       "                        [183, 130, 88],\n",
       "                        [95, 32, 0],\n",
       "                        [130, 114, 135],\n",
       "                        [110, 129, 133],\n",
       "                        [166, 74, 118],\n",
       "                        [219, 142, 185],\n",
       "                        [79, 210, 114],\n",
       "                        [178, 90, 62],\n",
       "                        [65, 70, 15],\n",
       "                        [127, 167, 115],\n",
       "                        [59, 105, 106],\n",
       "                        [142, 108, 45],\n",
       "                        [196, 172, 0],\n",
       "                        [95, 54, 80],\n",
       "                        [128, 76, 255],\n",
       "                        [201, 57, 1],\n",
       "                        [246, 0, 122],\n",
       "                        [191, 162, 208],\n",
       "                        [255, 255, 128],\n",
       "                        [147, 211, 203],\n",
       "                        [150, 100, 100],\n",
       "                        [168, 171, 172],\n",
       "                        [146, 112, 198],\n",
       "                        [210, 170, 100],\n",
       "                        [92, 136, 89],\n",
       "                        [218, 88, 184],\n",
       "                        [241, 129, 0],\n",
       "                        [217, 17, 255],\n",
       "                        [124, 74, 181],\n",
       "                        [70, 70, 70],\n",
       "                        [255, 228, 255],\n",
       "                        [154, 208, 0],\n",
       "                        [193, 0, 92],\n",
       "                        [76, 91, 113],\n",
       "                        [255, 180, 195],\n",
       "                        [106, 154, 176],\n",
       "                        [230, 150, 140],\n",
       "                        [60, 143, 255],\n",
       "                        [128, 64, 128],\n",
       "                        [92, 82, 55],\n",
       "                        [254, 212, 124],\n",
       "                        [73, 77, 174],\n",
       "                        [255, 160, 98],\n",
       "                        [255, 255, 255],\n",
       "                        [104, 84, 109],\n",
       "                        [169, 164, 131],\n",
       "                        [225, 199, 255],\n",
       "                        [137, 54, 74],\n",
       "                        [135, 158, 223],\n",
       "                        [7, 246, 231],\n",
       "                        [107, 255, 200],\n",
       "                        [58, 41, 149],\n",
       "                        [183, 121, 142],\n",
       "                        [255, 73, 97],\n",
       "                        [107, 142, 35],\n",
       "                        [190, 153, 153],\n",
       "                        [146, 139, 141],\n",
       "                        [70, 130, 180],\n",
       "                        [134, 199, 156],\n",
       "                        [209, 226, 140],\n",
       "                        [96, 36, 108],\n",
       "                        [96, 96, 96],\n",
       "                        [64, 170, 64],\n",
       "                        [152, 251, 152],\n",
       "                        [208, 229, 228],\n",
       "                        [206, 186, 171],\n",
       "                        [152, 161, 64],\n",
       "                        [116, 112, 0],\n",
       "                        [0, 114, 143],\n",
       "                        [102, 102, 156],\n",
       "                        [250, 141, 255]],\n",
       "          thing_dataset_id_to_contiguous_id={1: 0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup metadata \n",
    "metadata = Metadata()\n",
    "metadata.set(thing_classes=metadata_dict[\"thing_classes\"])\n",
    "metadata.set(thing_colors=metadata_dict[\"thing_colors\"])\n",
    "metadata.set(thing_dataset_id_to_contiguous_id=metadata_dict[\"thing_dataset_id_to_contiguous_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We specify the path of the configuration file, which contains all the settings of the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a configuration file\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(\"./model/config.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set model path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your .pth file path\n",
    "model_path = \"./model/model_final.pth\"\n",
    "cfg.MODEL.WEIGHTS = model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the device on which the model will run as CUDA and specify our threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.10  # set threshold for this model\n",
    "cfg.MODEL.DEVICE = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make the necessary definitions for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a predictor\n",
    "predictor = DefaultPredictor(cfg)\n",
    "loop = asyncio.get_event_loop()\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "executor = ThreadPoolExecutor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image = Image.open(\"./images.jpg\")\n",
    "image = numpy.asarray(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict started at  33:37.679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Users\\mbaru\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorShape.cpp:3484.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict finished at  33:39.391\n"
     ]
    }
   ],
   "source": [
    "# Use the predictor to make a prediction\n",
    "print(\"Predict started at \",getTime())\n",
    "outputs = predictor(image)\n",
    "print(\"Predict finished at \",getTime())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = outputs[\"instances\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167   238   896   476\n",
      "129496\n",
      "Volume ->  205   129496\n",
      "Mask Dimensions (mm) ->  {'x': 136, 'y': 48}\n",
      "113   253   460   373\n",
      "38000\n",
      "Volume ->  8   38000\n",
      "Mask Dimensions (mm) ->  {'x': 24, 'y': 23}\n",
      "119   236   308   509\n",
      "39758\n",
      "Volume ->  11   39758\n",
      "Mask Dimensions (mm) ->  {'x': -22, 'y': 56}\n",
      "{'results': [{'predictions': {'class_tag': 'bottle', 'score': 0.99, 'threshold': 0.1, 'mask_dimensions': {'x': 136, 'y': 48}, 'volume': 205, 'num_pixels': 129496}}, {'predictions': {'class_tag': 'bottle', 'score': 0.64, 'threshold': 0.1, 'mask_dimensions': {'x': 24, 'y': 23}, 'volume': 8, 'num_pixels': 38000}}, {'predictions': {'class_tag': 'bottle', 'score': 0.11, 'threshold': 0.1, 'mask_dimensions': {'x': -22, 'y': 56}, 'volume': 11, 'num_pixels': 39758}}], 'message': 'OK'}\n"
     ]
    }
   ],
   "source": [
    "results_list = []\n",
    "response = dict()\n",
    "for i in range(len(instances)):\n",
    "    prediction_dict = dict()\n",
    "\n",
    "    pred_boxes = instances.get(\"pred_boxes\")\n",
    "    if pred_boxes is None and len(pred_boxes.tensor) == 0:\n",
    "        print(\"No object detected!\")\n",
    "        response[\"message\"] = \"No object detected!\"\n",
    "        print(response)\n",
    "    elif i < len(instances):\n",
    "        pred = instances[i]\n",
    "        y1, x1, y2, x2 = pred.pred_boxes.tensor[0].tolist()\n",
    "        y1, x1, y2, x2 = int(y1), int(x1), int(y2), int(x2)  # convert to integers\n",
    "        print(y1, ' ', x1,' ',y2,' ',x2)\n",
    "        mask = pred.pred_masks[0][y1:y2, x1:x2].detach().cpu().numpy()\n",
    "        mask = mask.astype(int)\n",
    "        num_pixels = (pred.pred_masks[0].detach().cpu().numpy() == True).sum()\n",
    "        print(num_pixels)\n",
    "\n",
    "        # Toplam hacmi hesapla\n",
    "        volume, mask_dimensions_mm = calculateVolume(y1, x1, y2, x2, 135,num_pixels)\n",
    "        print(\"Volume -> \", volume,\" \", num_pixels)\n",
    "        print(\"Mask Dimensions (mm) -> \", mask_dimensions_mm)\n",
    "\n",
    "        # Retrieve the prediction score and threshold\n",
    "        scores = instances[i].scores.item()\n",
    "        threshold = cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST\n",
    "\n",
    "        # Retrieve the class tag (name)\n",
    "        class_label = instances[i].pred_classes.item()\n",
    "        class_tag = metadata.thing_classes[class_label]\n",
    "        prediction_dict[\"predictions\"] = {\n",
    "            \"class_tag\": class_tag,\n",
    "            \"score\": round(scores,2),\n",
    "            \"threshold\": threshold,\n",
    "            \"mask_dimensions\": mask_dimensions_mm,#{\"x\": round(x_dim,2), \"y\": round(y_dim,2)},\n",
    "            \"volume\": round(volume,1),\n",
    "            \"num_pixels\": int(num_pixels)\n",
    "            #\"prediction_image_base64\": prediction_base64\n",
    "        }\n",
    "        results_list.append(prediction_dict)\n",
    "        \n",
    "    else:\n",
    "        print(\"Index out of range for instances!\")\n",
    "        \n",
    "        response[\"message\"] = \"Index out of range for instances!\"\n",
    "        print(response)\n",
    "\n",
    "\n",
    "pred_boxes = instances.get(\"pred_boxes\")\n",
    "if pred_boxes is not None and len(pred_boxes.tensor) > 0:\n",
    "    draw_and_show_image(image,instances[0],\"filename\")\n",
    "response[\"results\"] = results_list\n",
    "response[\"message\"] = \"OK\"\n",
    "\n",
    "print(response)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detectron2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
